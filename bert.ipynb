{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils\n",
    "import requests\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from  data import create_dataset\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "MAX_LEN = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,                  \n",
    "                            add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "                            max_length=MAX_LEN+1,             # Adjust sentence length\n",
    "                            padding='max_length',    # Pad/truncate sentences\n",
    "                            return_attention_mask=True,# Generate attention masks\n",
    "                            return_tensors='pt',       # Return PyTorch tensors\n",
    "                            truncation = False\n",
    "                    )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        # Construct an attention mask (identifying padding/non-padding).\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    \n",
    "    return   input_ids,attention_masks \n",
    "\n",
    "def token(data, tokenizer):\n",
    "    input_a = []\n",
    "    input_b = []\n",
    "    input_c = []\n",
    "    input_d = []\n",
    "    for sentence in data:\n",
    "    \n",
    "        a,b,c,d = sentence.split()\n",
    "        input_a.append(a)\n",
    "        input_b.append(b)\n",
    "        input_c.append(c)\n",
    "        input_d.append(d)\n",
    "    input_idA, maskA = encode(tokenizer, input_a)\n",
    "    input_idB, maskB = encode(tokenizer, input_b)\n",
    "    input_idC, maskC = encode(tokenizer, input_c)\n",
    "    input_idD, maskD = encode(tokenizer, input_d)\n",
    "    \n",
    "    return input_idA, maskA, input_idB, maskB, input_idC, maskC, input_idD, maskD\n",
    "\n",
    "def embedding(model,tokens_tensor, attention_masks):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, attention_mask=attention_masks)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    #CLS contains all the information of the sentence so we use CLS token for embedding\n",
    "    CLS_embedding = hidden_states[:,0,:]\n",
    "    \n",
    "    return CLS_embedding\n",
    "\n",
    "\n",
    "def l2_dis(ab,c,d_prime, k =20):\n",
    "    candidates = []\n",
    "    for d in d_prime:\n",
    "        temp = ab +c-d\n",
    "\n",
    "        cos = torch.dot(temp,ab)/(torch.norm(temp,p=2)*torch.norm(ab,p=2))\n",
    "        candidates.append(cos)\n",
    "    \n",
    "    return  torch.topk(torch.Tensor(candidates), k, sorted=True, largest=False)\n",
    "\n",
    "def cos_sim(ab, c, d_prime, k=20):\n",
    "    candidates = []\n",
    "    for d in d_prime:\n",
    "        temp = ab +c-d\n",
    "        cos = torch.dot(temp,ab)/(torch.norm(temp,p=2)*torch.norm(ab,p=2))\n",
    "        candidates.append(cos)\n",
    "\n",
    "    return  torch.topk(torch.Tensor(candidates), k, sorted=True)\n",
    "def accuracy(predictions, words,k):\n",
    "    num = 0\n",
    "    for row in predictions:\n",
    "        if any(x == words[i] for i,x in enumerate(row[:k])):\n",
    "            num+=1\n",
    "    return num/len(words)\n",
    "\n",
    "\n",
    "def predicition(data,model,tokenizer, k_groups):\n",
    "    #Tokenize a,b,c,d and their coresponding masks\n",
    "    input_idA, maskA, input_idB, maskB, input_idC, maskC, input_idD, maskD = token(data,tokenizer)\n",
    "    #Add B and D together\n",
    "    \n",
    "    cd = torch.concat((input_idB,input_idD),0)\n",
    "    #maskUnique = torch.concat((maskB,maskD), 0)\n",
    "\n",
    "    #Find unique words as those are our candidates\n",
    "    unique_words = {tuple(tensor) for tensor in cd.tolist()}\n",
    "    unique_lst = [list(lst) for lst in unique_words]\n",
    "    unique_tensors = torch.stack([torch.tensor(list(lst)) for lst in unique_words])\n",
    "    unique_embedding = embedding(model,unique_tensors, maskD[:len(unique_lst)])\n",
    "    d = torch.Tensor(input_idD).tolist()\n",
    "    words = [ unique_lst.index(d[i]) for i in range(len(d))]\n",
    "    #sentence_embedding = embeddings(model, input_ids[:100], torch.tensor(attention_masks[:100]))\n",
    "    #find the embedding of a,b,c\n",
    "    embedding_A =  embedding(model, input_idA, maskA)\n",
    "    embedding_B =  embedding(model, input_idB, maskB)\n",
    "    embedding_C =  embedding(model, input_idC, maskC)\n",
    "    #embedding_D =  embedding(model, input_idD, maskD)\n",
    "\n",
    "    # For each sentence find the top k most likely words\n",
    "    pred_cos = []\n",
    "    pred_l2 = []\n",
    "    for i in range(embedding_A.shape[0]):\n",
    "        AB = embedding_A-embedding_B\n",
    "        top_k_cos, top_k_indicesC = cos_sim(AB[i], embedding_C[i],unique_embedding )\n",
    "        top_k_l2, top_k_indicesL = l2_dis(AB[i], embedding_C[i],unique_embedding )\n",
    "        #print(top_k_simil[0])\n",
    "        # for i in top_k_indices:\n",
    "        #     print(tokenizer.decode(unique_tensors[i], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        # print(torch.dot(AB[0],embedding_C[i] - embedding_D[i])/(torch.norm(AB[0],p=2)*torch.norm(embedding_C[i] - embedding_D[i],p=2)))\n",
    "        pred_cos.append(top_k_indicesC)\n",
    "        pred_l2.append(top_k_indicesL)\n",
    "    \n",
    "    for k in k_groups:\n",
    "        print(f'\\nk = {k}')\n",
    "        print(f'Cos Simmiliairty Accuracy : {100*accuracy(pred_cos,words,k):.4f} ')\n",
    "        print(f'L2 Accuracy :  {100*accuracy(pred_l2,words,k):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cs.fsu.edu/~liux/courses/deepRL/assignments/word-test.v1.txt'\n",
    "url_voca = 'https://www.cs.fsu.edu/~liux/courses/deepRL/assignments/bert_vocab.txt'\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', url_vocab= url_voca)\n",
    "model = BertModel.from_pretrained('bert-base-uncased' , output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "dataset = create_dataset(url)\n",
    "data = dataset['city-in-state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago Illinois Boston Massachusetts\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "i = 0\n",
    "index = -1\n",
    "for sen in data:\n",
    "    a,b,c,d = sen.split()\n",
    "    la = len(a)\n",
    "    lb = len(b)\n",
    "    lc = len(c)\n",
    "    ld = len(d)\n",
    "    size = max(la,lb,lc,ld)\n",
    "    if size> max_len:\n",
    "        index = i\n",
    "        max_len = size\n",
    "    i+=1\n",
    "print(data[index])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 1\n",
      "Cos Simmiliairty Accuracy : 7.7075 \n",
      "L2 Accuracy :  0.0000\n",
      "\n",
      "k = 2\n",
      "Cos Simmiliairty Accuracy : 8.3004 \n",
      "L2 Accuracy :  0.0000\n",
      "\n",
      "k = 5\n",
      "Cos Simmiliairty Accuracy : 28.6561 \n",
      "L2 Accuracy :  0.9881\n",
      "\n",
      "k = 10\n",
      "Cos Simmiliairty Accuracy : 45.8498 \n",
      "L2 Accuracy :  20.3557\n",
      "\n",
      "k = 20\n",
      "Cos Simmiliairty Accuracy : 65.0198 \n",
      "L2 Accuracy :  45.8498\n"
     ]
    }
   ],
   "source": [
    "groups = ('capital-common-countries')\n",
    "#groups = ('capital-common-countries', 'currency', 'city-in-state')\n",
    "\n",
    "k_group= (1,2,5,10,20)\n",
    "data = dataset['family']\n",
    "predicition(data,model,tokenizer, k_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
